---
title: "Advanced Inference: 2 - Nonlinear Regression"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
#devtools::install_github("rundel/learnrhash")

library(learnr)
library(tidyverse)
library(openintro)
library(grid)
library(png)
#library(emo)

knitr::opts_chunk$set(echo = FALSE,
                      fig.align = "center",
                      fig.height = 3,
                      fig.width = 5,
                      message = FALSE,
                      warning = FALSE)

tutorial_options(exercise.eval = FALSE)

# Hash generation helpers
# Should ideally be loaded from the imstutorials package when it exists
is_server_context <- function(.envir) {
  # We are in the server context if there are the follow:
  # * input - input reactive values
  # * output - shiny output
  # * session - shiny session
  #
  # Check context by examining the class of each of these.
  # If any is missing then it will be a NULL which will fail.
  
  inherits(.envir$input, "reactivevalues") &
    inherits(.envir$output, "shinyoutput") &
    inherits(.envir$session, "ShinySession")
}

check_server_context <- function(.envir) {
  if (!is_server_context(.envir)) {
    calling_func <- deparse(sys.calls()[[sys.nframe() - 1]])
    err <- paste0("Function `", calling_func, "`", " must be called from an Rmd chunk where `context = \"server\"`")
    stop(err, call. = FALSE)
  }
}
encoder_logic <- function(strip_output = FALSE) {
  p <- parent.frame()
  check_server_context(p)
  # Make this var available within the local context below
  assign("strip_output", strip_output, envir = p)
  # Evaluate in parent frame to get input, output, and session
  local(
    {
      encoded_txt <- shiny::eventReactive(
        input$hash_generate,
        {
          # shiny::getDefaultReactiveDomain()$userData$tutorial_state
          state <- learnr:::get_tutorial_state()
          shiny::validate(shiny::need(length(state) > 0, "No progress yet."))
          shiny::validate(shiny::need(nchar(input$name) > 0, "No name entered."))
          shiny::validate(shiny::need(nchar(input$studentID) > 0, "Please enter your student ID"))
          user_state <- purrr::map_dfr(state, identity, .id = "label")
          user_state <- dplyr::group_by(user_state, label, type, correct)
          user_state <- dplyr::summarize(
            user_state,
            answer = list(answer),
            timestamp = dplyr::first(timestamp),
            .groups = "drop"
          )
          user_state <- dplyr::relocate(user_state, correct, .before = timestamp)
          user_info <- tibble(
            label = c("student_name", "student_id"),
            type = "identifier",
            answer = as.list(c(input$name, input$studentID)),
            timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z", tz = "UTC")
          )
          learnrhash::encode_obj(bind_rows(user_info, user_state))
        }
      )
      output$hash_output <- shiny::renderText(encoded_txt())
    },
    envir = p
  )
}

hash_encoder_ui <- {
  shiny::div("If you have completed this tutorial and are happy with all of your", "solutions, please enter your identifying information, then click the button below to generate your hash", textInput("name", "What's your name?"), textInput("studentID", "What is your student ID?"), renderText({
    input$caption
  }), )
}
```

## Welcome

In this lesson, we'll learn how to model data that may not be linear. See the data table below:

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)  
library(ggplot2)

# Simulate some data 
x <- runif(n=100, min=0, max=10)
epsilon <- rnorm(n=100, mean=0, sd=1)
y <- 2*x**2 + epsilon*((1+x**2))

data <- data.frame(x,y)
head(data)
```


### Visualize data

Let's examine the data. First, generate a scatterplot of x and y.
```{r}
ggplot(data=data, aes(x=x, y=y)) +
  geom_point()
```

```{r mc1, echo=FALSE}
question("Is the relationship between x and y linear?",
         answer("Yes"),
         answer("No", correct=TRUE),
         answer("Not sure"))
```

Now let's look at the histograms of x and y, below. Notice that x looks more or less uniformly distributed, but y is highly skewed. This is another clue that the relationship between the variables is not linear.

```{r}
ggplot(data=data, aes(x=x)) +
  geom_histogram()

ggplot(data=data, aes(x=y)) +
  geom_histogram()
```


### Adding a transformation

Compute y_ = sqrt(y) and add it to the dataframe. Then, plot the relationship between x and y_.
```{r ex2, exercise=TRUE}
data$y_ <- _____

ggplot(data=data, aes(x=_____, y=_____)) +
  geom_point()
```

```{r ex2-solution}
data$y_ = sqrt(data$y)

ggplot(data=data, aes(x=x, y=y_)) +
  geom_point()
```

Does the relationship between x and y_ look linear?
```{r mc2, echo=FALSE}
question("Is the relationship between x and y_ linear?",
         answer("Yes", correct=TRUE),
         answer("No"),
         answer("Not sure"))
```


### Modeling the transformed data

Now, fit a linear model for x and y_. 

```{r ex3, exercise=TRUE}
model <- lm(______)
summary(model)
```

```{r ex3-solution}
model <- lm(data=data, y_ ~ x)
summary(model)
```


### Back-transformation

Now we need to back-transform the data in order to plot the best-fit line on
the original (non-transformed) domain of y. 

$y* = 0.557 + 1.268x + residuals$

$\sqrt{y}= 0.557 + 1.268x + residuals$

$y = (0.557 + 1.268x + residuals)^2$

The model on the last line is shown below.

```{r}
ggplot(data=data, aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~poly(x,2), se=FALSE)
```

Note: after transforming the data, you will need to take care when interpreting the coefficients of the model. As we showed, we cannot describe changes in $y$ as linear with respect to changes in $x$. Instead, in this specific case the change in $y$ is *quadratic* with changes in $x$. 


## Your turn!

Now try repeating the same analysis on MLB salary data. Here's a preview of what the dataset, called `Hitters`,  looks like:

```{r, echo=FALSE}
library(ISLR2)
data(Hitters)

head(Hitters)
```


Create a scatterplot of players' salaries vs RBI. RBI, coarsely speaking, measures how many points the team scores as a result of a player's at-bat. Check the linearity of the data with the scatterplot.

```{r ex4, exercise=TRUE}
```

```{r ex4-solution}
ggplot(data=Hitters, aes(x=RBI, y=Salary)) +
  geom_point()
```


### A Different Transformation

So the Salaries are not linear, and you'll have to transform them again. Take another look at the scatterplot above.

```{r mc3, echo=FALSE}
question("What relationship does MLB players' salary and RBI seem to have?",
         answer("Linear"),
         answer("Exponential", correct=TRUE),
         answer("Cubic"),
         answer("Not sure"))
```


### Modeling Exponential Data

Perform a log transformation on the Salary variable, and add it to the dataframe. Then use the transformed variable in a linear model as you did with the quadratic example above.

```{r ex5, exercise=TRUE}
Hitters$logSalary <- _____
```

```{r ex5-solution}
Hitters$logSalary <- log(Hitters$Salary)
```

```{r mc4, echo=FALSE}
question("What inverse operation will you need to perform on the model for the correct back-transformation?",
         answer("(...)^2"),
         answer("exp(...) or e^(...)", correct=TRUE),
         answer("sqrt(...)"),
         answer("1/(...)"))
```


### Back Transformation

After back-transforming, your model will be:

$y = e^{\beta_0 + \beta_1x_{RBI} + residuals}$


### Interpretation

Now on a log scale, `logSalary` appears linear with RBI:

```{r, include=FALSE}
Hitters$logSalary <- log(Hitters$Salary)
```

```{r}
ggplot(data=Hitters, aes(x=RBI, y=logSalary)) +
  geom_point() +
  geom_smooth(method="lm", formula=(y~x), se=FALSE, color="blue")
```

Note the difference in the scales on the y-axis compared to the first scatterplot you made, because now we're on a log scale. 


## Submit

```{r, echo=FALSE, context="server"}
encoder_logic()
```

```{r encode, echo=FALSE}
learnrhash::encoder_ui(ui_before = hash_encoder_ui)
```
