---
title: "Bayesian Foundations: 1 - Review"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
#devtools::install_github("rundel/learnrhash")
#devtools::install_git("git@github.com:rundel/learnrhash.git")
library(learnr)
library(tidyverse)
library(openintro)
library(grid)
library(png)
library(ggplot2)
#library(emo)
knitr::opts_chunk$set(echo = FALSE,
                      fig.align = "center",
                      fig.height = 3,
                      fig.width = 5,
                      message = FALSE,
                      warning = FALSE)
tutorial_options(exercise.eval = FALSE)
# Hash generation helpers
# Should ideally be loaded from the imstutorials package when it exists
is_server_context <- function(.envir) {
  # We are in the server context if there are the follow:
  # * input - input reactive values
  # * output - shiny output
  # * session - shiny session
  #
  # Check context by examining the class of each of these.
  # If any is missing then it will be a NULL which will fail.
  
  inherits(.envir$input, "reactivevalues") &
    inherits(.envir$output, "shinyoutput") &
    inherits(.envir$session, "ShinySession")
}
check_server_context <- function(.envir) {
  if (!is_server_context(.envir)) {
    calling_func <- deparse(sys.calls()[[sys.nframe() - 1]])
    err <- paste0("Function `", calling_func, "`", " must be called from an Rmd chunk where `context = \"server\"`")
    stop(err, call. = FALSE)
  }
}
encoder_logic <- function(strip_output = FALSE) {
  p <- parent.frame()
  check_server_context(p)
  # Make this var available within the local context below
  assign("strip_output", strip_output, envir = p)
  # Evaluate in parent frame to get input, output, and session
  local(
    {
      encoded_txt <- shiny::eventReactive(
        input$hash_generate,
        {
          # shiny::getDefaultReactiveDomain()$userData$tutorial_state
          state <- learnr:::get_tutorial_state()
          shiny::validate(shiny::need(length(state) > 0, "No progress yet."))
          shiny::validate(shiny::need(nchar(input$name) > 0, "No name entered."))
          shiny::validate(shiny::need(nchar(input$studentID) > 0, "Please enter your student ID"))
          user_state <- purrr::map_dfr(state, identity, .id = "label")
          user_state <- dplyr::group_by(user_state, label, type, correct)
          user_state <- dplyr::summarize(
            user_state,
            answer = list(answer),
            timestamp = dplyr::first(timestamp),
            .groups = "drop"
          )
          user_state <- dplyr::relocate(user_state, correct, .before = timestamp)
          user_info <- tibble(
            label = c("student_name", "student_id"),
            type = "identifier",
            answer = as.list(c(input$name, input$studentID)),
            timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z", tz = "UTC")
          )
          learnrhash::encode_obj(bind_rows(user_info, user_state))
        }
      )
      output$hash_output <- shiny::renderText(encoded_txt())
    },
    envir = p
  )
}
hash_encoder_ui <- {
  shiny::div("If you have completed this tutorial and are happy with all of your", "solutions, please enter your identifying information, then click the button below to generate your hash", textInput("name", "What's your name?"), textInput("studentID", "What is your student ID?"), renderText({
    input$caption
  }), )
}
```

## Welcome

In this tutorial, we'll take a quick break from modeling to review probabilities, events, and conditional probability. This will help us build up a foundation for thinking of statistical problems in a Bayesian way. 


## Probability Review

Interpreting probability in a Bayesian way may be new to you (at least in Statistics classes). Consider the classic setting of flipping a fair coin. Then the probability of getting heads is $1/2$, and the probability of getting tails is also $1/2$. There are (at least) two ways of interpreting this probability:

1. If we flip the coin over and over, we will get heads roughly $1/2$ the time, and tails roughly $1/2$ the time.
2. The two outcomes (heads and tails) are equally likely.

###

The former interpretation is often attributed to a *Frequentist* way of thinking, while the second is more *Bayesian*. 

For another example, say that two students are preparing for a test and guessing at their probability of passing. The first student is very confident in their preparation, and says that their probability of passing is 0.9. The second student feels less confident, and says 0.6. 

###

A Frequentist interpretation says that, if the students could each take the exam many times over and over again, they would pass 90% or 60% of the time, respectively. 

###

In fact, there's an even stricter interpretation: assuming the students can only take the exam once, this is a binary event --- so their probability of passing is either 0 or 1. That is, both students are wrong!

This may raise some eyebrows for you (or not). Do you find this way of thinking about probabilities intuitive?

###

On the other hand, a more Bayesian interpretation of the probabilities says that the first student is much more likely to pass than to fail, and the second student is only a bit more likely to pass than to fail. 

Their estimates of these probabilities are based on how much they've each prepared for the exam, what they know about the class, the professor, and/or the exam itself, and how they think they'll be evaluated. In other words, the students each have some prior knowledge and they each make an informed guess based on that knowledge. This, in essence, is the Bayesian approach to statistics. More on that in a bit!

###

These are just two ways of interpreting probabilities. Which one feels more intuitive to you will depend on how you think, and you may even use one or the other (or both) when approaching different problems. You'll notice that the Frequentist philosophy is more explicitly taught in Statistics classes, including in this one (so far!). Starting from now, we will explore the Bayesian way. 

###

## Big Picture

In the exam example above, we introduced the idea of a "Bayesian approach" to statistical problems. Let's describe this approach in more detail. 

Imagine that you are one of the two students studying for an exam. Say that, for the same class, you already took one other exam and you passed with a grade of 80%. Let's call this your _prior_: given your typical study methods, you can confidently pass an exam for this class. 

###

Now, you sit for the exam, and unfortunately it goes poorly. You missed something in your preparations or just had a bad day, and when you get the grade back it is now a 60%. Still a pass, but much lower than you had hoped. Here's your _new data_: in your most recent exam, you performed worse than before.

###

Your class has one more exam left in the semester. Given your past two performances, you know you can do well on the material in the beginning of the class, but you'll need to review the second exam material. You make a plan to adjust your study methods and take care of yourself outside of class, and you hope to get at least a 75% on the final exam. This is your _posterior_. 

###

This method of building up knowledge (prior -> data -> posterior) is the nature of Bayesian analysis (see the figure below).

![Figure 1: Bayesian knowledge-building at work. Source: BayesRules! (Chapter 1.1)](./bayes_diagram.png)

In the coming sections we will formalize these ideas in terms of statistical objects like probabilities and distributions. But first, let's continue the review.


## Events Review

Often when we study a problem in statistics, we are interested in an _outcome_. For example, we may want to know how likely an event is to happen or not happen, whether in terms of a discrete probability or with a distribution. To do this, we build models that help us understand the underlying structure of data we have observed - which tells us about the events we are interested in. 


Outline:
1. Introduce diagnostic test background (true/false pos/neg, etc.)
2. Work out the probabilities of each event
3. MC questions for computing probabilities of events in different problems


## Conditional Probability Review

Outline:
1. Same diagnostic test, now we have a result (patient goes to doc., etc.)
2. Given the result, what is the probability of each event?
3. Derive Bayes' Rule?? Or maybe just use it to answer (2)
4. MC questions for using BR in different problems


## Submit

```{r, echo=FALSE, context="server"}
encoder_logic()
```

```{r encode, echo=FALSE}
learnrhash::encoder_ui(ui_before = hash_encoder_ui)
```
