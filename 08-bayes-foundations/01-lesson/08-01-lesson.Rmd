---
title: "Bayesian Foundations: 1 - Review"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
#devtools::install_github("rundel/learnrhash")
#devtools::install_git("git@github.com:rundel/learnrhash.git")
library(learnr)
library(tidyverse)
library(openintro)
library(grid)
library(png)
library(ggplot2)
#library(emo)
knitr::opts_chunk$set(echo = FALSE,
                      fig.align = "center",
                      fig.height = 3,
                      fig.width = 5,
                      message = FALSE,
                      warning = FALSE)
tutorial_options(exercise.eval = FALSE)
# Hash generation helpers
# Should ideally be loaded from the imstutorials package when it exists
is_server_context <- function(.envir) {
  # We are in the server context if there are the follow:
  # * input - input reactive values
  # * output - shiny output
  # * session - shiny session
  #
  # Check context by examining the class of each of these.
  # If any is missing then it will be a NULL which will fail.
  
  inherits(.envir$input, "reactivevalues") &
    inherits(.envir$output, "shinyoutput") &
    inherits(.envir$session, "ShinySession")
}
check_server_context <- function(.envir) {
  if (!is_server_context(.envir)) {
    calling_func <- deparse(sys.calls()[[sys.nframe() - 1]])
    err <- paste0("Function `", calling_func, "`", " must be called from an Rmd chunk where `context = \"server\"`")
    stop(err, call. = FALSE)
  }
}
encoder_logic <- function(strip_output = FALSE) {
  p <- parent.frame()
  check_server_context(p)
  # Make this var available within the local context below
  assign("strip_output", strip_output, envir = p)
  # Evaluate in parent frame to get input, output, and session
  local(
    {
      encoded_txt <- shiny::eventReactive(
        input$hash_generate,
        {
          # shiny::getDefaultReactiveDomain()$userData$tutorial_state
          state <- learnr:::get_tutorial_state()
          shiny::validate(shiny::need(length(state) > 0, "No progress yet."))
          shiny::validate(shiny::need(nchar(input$name) > 0, "No name entered."))
          shiny::validate(shiny::need(nchar(input$studentID) > 0, "Please enter your student ID"))
          user_state <- purrr::map_dfr(state, identity, .id = "label")
          user_state <- dplyr::group_by(user_state, label, type, correct)
          user_state <- dplyr::summarize(
            user_state,
            answer = list(answer),
            timestamp = dplyr::first(timestamp),
            .groups = "drop"
          )
          user_state <- dplyr::relocate(user_state, correct, .before = timestamp)
          user_info <- tibble(
            label = c("student_name", "student_id"),
            type = "identifier",
            answer = as.list(c(input$name, input$studentID)),
            timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z", tz = "UTC")
          )
          learnrhash::encode_obj(bind_rows(user_info, user_state))
        }
      )
      output$hash_output <- shiny::renderText(encoded_txt())
    },
    envir = p
  )
}
hash_encoder_ui <- {
  shiny::div("If you have completed this tutorial and are happy with all of your", "solutions, please enter your identifying information, then click the button below to generate your hash", textInput("name", "What's your name?"), textInput("studentID", "What is your student ID?"), renderText({
    input$caption
  }), )
}
```

## Welcome

In this tutorial, we'll take a quick break from modeling to review probabilities, events, and conditional probability. This will help us build up a foundation for thinking of statistical problems in a Bayesian way. 


## Probability Review

Interpreting probability in a Bayesian way may be new to you (at least in Statistics classes). Consider the classic setting of flipping a fair coin. Then the probability of getting heads is $1/2$, and the probability of getting tails is also $1/2$. There are (at least) two ways of interpreting this probability:

1. If we flip the coin over and over, we will get heads roughly $1/2$ the time, and tails roughly $1/2$ the time.
2. The two outcomes (heads and tails) are equally likely.

###

The former interpretation is often attributed to a *Frequentist* way of thinking, while the second is more *Bayesian*. 

For another example, say that two students are preparing for a test and guessing at their probability of passing. The first student is very confident in their preparation, and says that their probability of passing is 0.9. The second student feels less confident, and says 0.6. 

###

A Frequentist interpretation says that, if the students could each take the exam many times over and over again, they would pass 90% or 60% of the time, respectively. 

###

In fact, there's an even stricter interpretation: assuming the students can only take the exam once, this is a binary event --- so their probability of passing is either 0 or 1. That is, both students are wrong!

This may raise some eyebrows for you (or not). Do you find this way of thinking about probabilities intuitive?

###

On the other hand, a more Bayesian interpretation of the probabilities says that the first student is much more likely to pass than to fail, and the second student is only a bit more likely to pass than to fail. 

Their estimates of these probabilities are based on how much they've each prepared for the exam, what they know about the class, the professor, and/or the exam itself, and how they think they'll be evaluated. In other words, the students each have some prior knowledge and they each make an informed guess based on that knowledge. This, in essence, is the Bayesian approach to statistics. More on that in a bit!

###

These are just two ways of interpreting probabilities. Which one feels more intuitive to you will depend on how you think, and you may even use one or the other (or both) when approaching different problems. You'll notice that the Frequentist philosophy is more explicitly taught in Statistics classes, including in this one (so far!). Starting from now, we will explore the Bayesian way. 

###

## Big Picture

In the exam example above, we introduced the idea of a "Bayesian approach" to statistical problems. Let's describe this approach in more detail. 

Imagine that you are one of the two students studying for an exam. Say that, for the same class, you already took one other exam and you passed with a grade of 80%. Let's call this your _prior_: given your typical study methods, you can confidently pass an exam for this class. 

###

Now, you sit for the exam, and unfortunately it goes poorly. You missed something in your preparations or just had a bad day, and when you get the grade back it is now a 60%. Still a pass, but much lower than you had hoped. Here's your _new data_: in your most recent exam, you performed worse than before.

###

Your class has one more exam left in the semester. Given your past two performances, you know you can do well on the material in the beginning of the class, but you'll need to review the second exam material. You make a plan to adjust your study methods and take care of yourself outside of class, and you hope to get at least a 75% on the final exam. This is your _posterior_. 

###

This method of building up knowledge (prior -> data -> posterior) is the nature of Bayesian analysis (see the figure below).

![Figure 1: Bayesian knowledge-building at work. Source: BayesRules! (Chapter 1.1)](./bayes_diagram.png)

In the coming sections we will formalize these ideas in terms of statistical objects like probabilities and distributions. But first, let's continue the review.


## Events Review

Often when we study a problem in statistics, we are interested in an _outcome_. For example, we may want to know how likely an event is to happen or not happen. One such problem might go something like this: say that a given disease affects $0.5%$ of the US population. A test is developed to detect the disease, and experiments say that the test gives false positives $3%$ of the time, and false negatives $2%$ of the time. Then we can write out the events like this:

$$\mathbb{P}(FP) = 0.03, \quad \mathbb{P}(FN) = 0.02, \quad \text{and} \quad \mathbb{P}(D) = 0.005$$
where $D$ is the event that a random person has the disease, $FP$ is the event that the diagnostic test returns a _false positive_ result (where the result is positive but the patient in fact does not have the disease), and $FN$ is the event that the test returns a _false negative_ (where the result is negative but the patient does in fact have the disease).

From the information given by the problem, we can also determine the probabilities of other events in the _sample spaces_, the full spaces of possibilities. 

###

```{r mc1}
question("Which of the following is the correct description of the event FP^c (the complement of event FP)?",
         answer("FP^c is the same thing as FN (false negative)"),
         answer("FP^c is when the diagnostic test gives a positive result, and the patient does in fact have the disease (true positive)", correct=TRUE),
         answer("FP^c is when the diagnostic test gives a negative result."),
         answer("FP^c is when the diagnostic test gives a positive result."),
         answer("Not sure"))
```

###

```{r mc2}
question("Which of the following is the correct description of the event FN^c (the complement of event FN)?",
         answer("FN^c is the same thing as FP (false positive)"),
         answer("FN^c is when the diagnostic test gives a negative result, and the patient does not have the disease (true negative)", correct=TRUE),
         answer("FN^c is when the diagnostic test gives a negative result."),
         answer("FN^c is when the diagnostic test gives a positive result."),
         answer("Not sure"))
```

###

And finally, we also have

$$D^c = \{\text{a random person in the US does not have the disease}\}$$
Which has probability...

```{r mc3}
question("What is the probability of the event D^c (the complement of event D)?",
         answer("1-0.005, or 0.995", correct=TRUE),
         answer("0.005"),
         answer("0.03"),
         answer("Not sure"))
```

#$$ \mathbb{P}(D^c) = 1-\mathbb{P}(D) = 1-0.005 = 0.995$$


## Conditional Probability and Bayes' Rule Review 

Now we'll extend the example. Say that a patient goes to the doctor for the diagnostic test and receives a positive test result. We want to work out: what is the probability that the patient truly has the disease? In other words, we need to compute the probability of the conditional event $D|P$.

#$$\mathbb{P}(TP) = \mathbb{P}(\text{the test result is positive and the patient does have the disease}) = \text{?}$$

###

Recall that the _conditional probability_ of an event A _given_ B, is

$$\mathbb{P}(A|B) = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}$$ 

Similarly, we have that the conditional probability of an event B given A is

$$\mathbb{P}(B|A) = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}$$ 

Notice that we have $\mathbb{P}(A\cap B)$ in the numerator for both quantities, so we can solve for it to get 

$$\mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(A\cap B) = \mathbb{P}(B|A)\mathbb{P}(A)$$

And substitute back into the first equation to get

$$\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}$$

###

So, back to the problem at hand. We want to determine $\mathbb{P}(D|P)$, where event P is the positive test result. But what is $\mathbb{P}(P)$?

***TODO: finish question***

```{r mc4}
question("(Challenge) How else can you express the probability of event P? Hint: remember the law of total probability.",
         answer("The probability of P is ", correct=TRUE), 
         answer("The probability of P is the same thing as the probability of D"),
         answer("The probability of P is the sum of the probabilities of FP (false positive) and D (disease rate)"),
         answer("The probability of P is the sum of the probabilities of FP (false positive) and FN (false negative)."),
         answer("Not sure"))
```

###

The key to answering the above question is the Law of Total Probability. First, note that our disease sample space can be *partitioned* into two *disjoint*, *mutually exclusive*, and *exhaustive* events: $D$ and $D^c$. Why are these two events mutually exclusive and exhaustive?

###

Then the Law of Total Probability states that

$$\mathbb{P}(P) = \mathbb{P}(P \cap D) + \mathbb{P}(P \cap D^c) \\ = \mathbb{P}(P|D)\mathbb{P}(D) + \mathbb{P}(P|D^c)\mathbb{P}(D^c)$$
where the second line follows from applying the rules of conditional probability.

Now, what is $\mathbb{P}(P|D)$, or in other words the probability of a _true positive_ test result?

###

```{r mc5}
question("How else can you express the event TP, or equivalently, P|D?",
         answer("TP (true positive) is the same as P."),
         answer("TP (true positive) can also be expressed as FN^c, the complement of FN (false negative).", correct=TRUE), 
         answer("TP (true positive) can also be expressed as FP^c, the complement of FP (false positive)."),
         answer("Not sure"))
```

###

Now we can compute $\mathbb{P}(TP)$:

```{r mc6}
question("What is the probability of TP?",
         answer("TP (true positive) is 1-0.03 = 0.97."),
         answer("The probability of TP (true positive) is 0.005."),
         answer("The probability of TP (true positive) is 1-0.02 = 0.98.", correct=TRUE), 
         answer("The probability of TP (true positive) is 1-0.005 = 0.995."),
         answer("Not sure"))
```

###

Finally, we can put the pieces together and solve our problem:

$$\mathbb{P}(D|P) = \frac{\mathbb{P}(P|D)\mathbb{P}(D)}{\mathbb{P}(P)} = \frac{\mathbb{P}(TP)\mathbb{P}(D)}{\mathbb{P}(TP)\mathbb{P}(D) + \mathbb{P}(FP)\mathbb{P}(D^c)} \\ = \frac{(1-0.02)(0.005)}{(1-0.02)(0.005) + (0.03)(1-0.005)} = 0.141$$

## False Positive Paradox

In this section, you will repeat the previous exercise in a new problem to explore what is known as the False Positive Paradox. Imagine we have the same setup, except this time we are dealing with a disease that is far more prevalent in the general population: $\mathbb{P}(D) = 0.4$. The other givens are the same, namely $\mathbb{P}(FP) = 0.03$ and $\mathbb{P}(0.02)$. As before, you'll have to use conditional probability, Bayes' Rule, and the Law of Total Probability. 


***TODO: finish - maybe use R as a fill-in-the-blanks interface?***


## Submit

```{r, echo=FALSE, context="server"}
encoder_logic()
```

```{r encode, echo=FALSE}
learnrhash::encoder_ui(ui_before = hash_encoder_ui)
```
